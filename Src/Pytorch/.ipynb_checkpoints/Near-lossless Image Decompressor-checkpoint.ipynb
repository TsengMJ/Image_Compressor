{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  5159\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Unit_GN(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, padding):\n",
    "        super(Residual_Unit_GN, self).__init__()\n",
    "        \n",
    "        self.Conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, \n",
    "                               kernel_size=kernel_size, stride=1, padding=padding) \n",
    "        self.Conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, \n",
    "                               kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.BN1   = nn.BatchNorm2d(num_features=channels)\n",
    "        self.BN2   = nn.BatchNorm2d(num_features=channels)\n",
    "        self.ReLU  = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.Conv1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.ReLU(out)\n",
    "        out = self.Conv2(out)\n",
    "        out = self.BN2(out)\n",
    "        out = out + identity\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class Conv_Unit_DN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, strides_1, strides_2, padding):\n",
    "        super(Conv_Unit_DN, self).__init__()\n",
    "        \n",
    "        self.Conv1  = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                kernel_size=kernel_size, stride=strides_1, padding=padding)\n",
    "        self.Conv2  = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                                kernel_size=kernel_size, stride=strides_2, padding=padding)\n",
    "        self.BN1    = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.BN2    = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.LReLU1 = nn.LeakyReLU()\n",
    "        self.LReLU2 = nn.LeakyReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.Conv1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.LReLU1(out)\n",
    "        out = self.Conv2(out)\n",
    "        out = self.BN2(out)\n",
    "        out = self.LReLU2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class Residual_Unit_DN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super(Residual_Unit_DN, self).__init__()\n",
    "        \n",
    "        first_channels = int(out_channels/2)\n",
    "        \n",
    "        self.Conv1  = nn.Conv2d(in_channels=in_channels   , out_channels=first_channels, \n",
    "                        kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.Conv2  = nn.Conv2d(in_channels=first_channels, out_channels=out_channels, \n",
    "                        kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.BN1    = nn.BatchNorm2d(num_features=first_channels)\n",
    "        self.BN2    = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.LReLU1 = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.Conv1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.LReLU1(out)\n",
    "        out = self.Conv2(out)\n",
    "        out = self.BN2(out)\n",
    "        out = out + identity\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.Conv1   = nn.Conv2d(in_channels=1 , out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.Conv2   = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.Conv3   = nn.Conv2d(in_channels=64, out_channels=1 , kernel_size=3, stride=1, padding=1)\n",
    "        self.ReLU    = nn.ReLU()\n",
    "        self.BN      = nn.BatchNorm2d(num_features=64)\n",
    "        self.Tanh    = nn.Tanh()\n",
    "        self.RUnit1  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit2  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit3  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit4  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit5  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit6  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit7  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit8  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit9  = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit10 = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit11 = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit12 = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit13 = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit14 = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit15 = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        self.RUnit16 = Residual_Unit_GN(channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.Conv1(x)\n",
    "        out = self.ReLU(out)\n",
    "        identical = out\n",
    "        \n",
    "        out = self.RUnit1(out)\n",
    "        out = self.RUnit2(out)\n",
    "        out = self.RUnit3(out)\n",
    "        out = self.RUnit4(out)\n",
    "        out = self.RUnit5(out)\n",
    "        out = self.RUnit6(out)\n",
    "        out = self.RUnit7(out)\n",
    "        out = self.RUnit8(out)\n",
    "        out = self.RUnit9(out)\n",
    "        out = self.RUnit10(out)\n",
    "        out = self.RUnit11(out)\n",
    "        out = self.RUnit12(out)\n",
    "        out = self.RUnit13(out)\n",
    "        out = self.RUnit14(out)\n",
    "        out = self.RUnit15(out)\n",
    "        out = self.RUnit16(out)\n",
    "        out = self.Conv2(out)\n",
    "        out = self.BN(out)\n",
    "        out = out + identical\n",
    "        out = self.Conv3(out)\n",
    "        out = self.Tanh(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.Conv    = nn.Conv2d(in_channels=1 , out_channels=32, kernel_size=3, stride=1 ,padding=1)\n",
    "        self.LReLU1  = nn.LeakyReLU()\n",
    "        self.LReLU2  = nn.LeakyReLU()\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.Dense   = nn.Linear(in_features=128, out_features=2)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.CUnit1  = Conv_Unit_DN(in_channels=32 , out_channels=64 , kernel_size=3, \n",
    "                                    strides_1=1, strides_2=2, padding=1)\n",
    "        self.CUnit2  = Conv_Unit_DN(in_channels=64 , out_channels=128, kernel_size=3, \n",
    "                                    strides_1=1, strides_2=1, padding=1)\n",
    "        self.CUnit3  = Conv_Unit_DN(in_channels=128, out_channels=128, kernel_size=3, \n",
    "                                    strides_1=1, strides_2=2, padding=1)\n",
    "        self.CUnit4  = Conv_Unit_DN(in_channels=128, out_channels=256, kernel_size=3, \n",
    "                                    strides_1=1, strides_2=1, padding=1)\n",
    "        self.CUnit5  = Conv_Unit_DN(in_channels=256, out_channels=256, kernel_size=3, \n",
    "                                    strides_1=1, strides_2=2, padding=1)\n",
    "        self.CUnit6  = Conv_Unit_DN(in_channels=256, out_channels=512, kernel_size=3, \n",
    "                                    strides_1=1, strides_2=2, padding=1)\n",
    "        self.RUnit1  = Residual_Unit_DN(in_channels=512, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.Conv(x)\n",
    "        out = self.LReLU1(out)\n",
    "        out = self.CUnit1(out)\n",
    "        out = self.CUnit2(out)\n",
    "        out = self.CUnit3(out)\n",
    "        out = self.CUnit4(out)\n",
    "        out = self.CUnit5(out)\n",
    "        out = self.CUnit6(out)\n",
    "        out = self.RUnit1(out)\n",
    "        out = self.LReLU2(out)\n",
    "        out = self.Flatten(out)\n",
    "        out = self.Dense(out)\n",
    "        out = self.Sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "#         return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.path_dcp = os.path.join(root_dir, \"Decompressed\")\n",
    "        self.path_ori = os.path.join(root_dir, \"Original_pgm\")\n",
    "        \n",
    "        self.dcp_name = os.listdir(self.path_dcp)\n",
    "        self.dcp_name.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dcp_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_dcp_loc = os.path.join(self.path_dcp, self.dcp_name[idx])\n",
    "        img_ori_loc = os.path.join(self.path_ori, self.dcp_name[idx])\n",
    "        img_dcp = cv2.imread(img_dcp_loc, 0).reshape(1,64,64)\n",
    "        img_ori = cv2.imread(img_ori_loc, 0).reshape(1,64,64)\n",
    "        sample = {'Dcp': img_dcp, 'Ori': img_ori}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Models\n",
      "[*] Initialized Parameters\n",
      "[*] Start Training Generative Network\n",
      "[0/5][0.0000]\tLoss_D: 16522.7949\n",
      "[0/5][0.0003]\tLoss_D: 14125.6680\n",
      "[0/5][0.0006]\tLoss_D: 14385.7305\n",
      "[0/5][0.0009]\tLoss_D: 16061.1299\n",
      "[0/5][0.0013]\tLoss_D: 18085.2363\n",
      "[0/5][0.0016]\tLoss_D: 17520.2695\n",
      "[0/5][0.0019]\tLoss_D: 14496.9287\n",
      "[0/5][0.0022]\tLoss_D: 17634.6016\n",
      "[0/5][0.0025]\tLoss_D: 16113.0693\n",
      "[0/5][0.0028]\tLoss_D: 20487.5352\n",
      "[0/5][0.0031]\tLoss_D: 17034.4609\n",
      "[0/5][0.0034]\tLoss_D: 16957.4570\n",
      "[0/5][0.0038]\tLoss_D: 16945.7969\n",
      "[0/5][0.0041]\tLoss_D: 14388.5361\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "Workers = 64\n",
    "BatchSize = 64\n",
    "num_epochs = 5\n",
    "LR = 0.0002\n",
    "Beta1 = 0.8\n",
    "NGpu = 1\n",
    "\n",
    "\n",
    "# Step 1: 讀取訓練資料（受損 / 原圖）\n",
    "train_path = \"/home/mj/HardDisk/Github/Image_Compressor/Dataset/Training_Data\"  ## 受損影像路徑\n",
    "# valid_path = \"../../Dataset/Validation_Data\"  ## 原圖影像路徑\n",
    "train_data_loader = DataLoader(Image(root_dir = train_path), batch_size=BatchSize, shuffle=True, num_workers=Workers)\n",
    "# valid_data_loader = DataLoader(Image(valid_path), batch_size=BatchSize, shuffle=True, num_workers=Workers)\n",
    "\n",
    "\n",
    "# Step 2: 讀取 Models\n",
    "print(\"[*] Loading Models\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG = Generator(NGpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "# print(netG)\n",
    "\n",
    "netD = Discriminator(NGpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "# print(netD)\n",
    "\n",
    "\n",
    "# Step 3: 初始化參數配置\n",
    "print(\"[*] Initialized Parameters\")\n",
    "optimizer_G = torch.optim.Adam(netG.parameters(), lr=LR, betas=(Beta1, 0.999))  \n",
    "loss_MSE = nn.MSELoss()\n",
    "\n",
    "# Step : 先稍微訓練 Generative Network\n",
    "torch.cuda.empty_cache()\n",
    "print(\"[*] Start Training Generative Network\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_data_loader, 0):\n",
    "        # clear out the gradients of all Variables\n",
    "        optimizer_G.zero_grad()\n",
    "        dcp, ori = data['Dcp'].to(device, dtype=torch.float, non_blocking=True, copy=False), data['Ori'].to(device, dtype=torch.float, non_blocking=True, copy=False)\n",
    "        \n",
    "        output = netG(dcp)\n",
    "        loss = loss_MSE(output, ori)\n",
    "        loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('[%d/%d][%.4f]\\tLoss_D: %.4f'% (epoch, num_epochs, i/len(train_data_loader), loss.item()))        \n",
    "        \n",
    "#         if i % 10 == 0:\n",
    "#             print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "#                   % (epoch, num_epochs, i, len(dataloader),\n",
    "#                      errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "\n",
    "# Step : 加入 Discriminiative Network 一起訓練， 儲存權重\n",
    "\n",
    "# Step : 儲存 Final 權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 歷史的眼淚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Residual_Unit_GN(x):\n",
    "#     ## Connect Layers\n",
    "#     identity = x\n",
    "\n",
    "#     out = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)(x)\n",
    "#     out = nn.BatchNorm2d(num_features=64)(out)\n",
    "#     out = nn.ReLU()(out)\n",
    "#     out = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)(out)\n",
    "#     out = nn.BatchNorm2d(num_features=64)(out)\n",
    "#     out = out + identity\n",
    "\n",
    "#     return out\n",
    "\n",
    "# def Conv_Unit_DN(x, in_channels, out_channels, strides_1, strides_2):\n",
    "#     out = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=strides_1)(x)\n",
    "#     out = nn.BatchNorm2d(num_features=out_channels_1)(out)\n",
    "#     out = nn.LeakyReLU()(out)\n",
    "#     out = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=strides_2)(x)\n",
    "#     out = nn.BatchNorm2d(num_features=out_channels_1)(out)\n",
    "#     out = nn.LeakyReLU()(out)\n",
    "    \n",
    "#     return out\n",
    "    \n",
    "# def Residual_Unit_DN(x):\n",
    "#     identity = x\n",
    "    \n",
    "#     out = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=3, stride=1)(x)\n",
    "#     out = nn.BatchNorm2d(num_features=64)(out)\n",
    "#     out = nn.LeakyReLU()(out)\n",
    "#     out = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1)(out)\n",
    "#     out = nn.BatchNorm2d(num_features=64)(out)\n",
    "#     out = out + identity\n",
    "    \n",
    "#     return out "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
