{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  5845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "%run Models.ipynb\n",
    "%run LossFunctions.ipynb\n",
    "%run DataLoader.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: 宣告訓練參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "Workers = 4\n",
    "BatchSize = 32\n",
    "LR = 0.0001\n",
    "Beta1 = 0.9\n",
    "NGpu = 1\n",
    "Model_path = \"/home/mj/HardDisk/Github/Image_Compressor/Model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: 讀取訓練資料（受損 / 原圖）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/mj/HardDisk/Github/Image_Compressor/Dataset/Training_Data\"  ## 受損影像路徑\n",
    "# valid_path = \"/home/mj/HardDisk/Github/Image_Compressor/Dataset/Validation_Data\"  ## 原圖影像路徑\n",
    "\n",
    "train_data_loader = DataLoader(Image(train_path), batch_size=BatchSize, shuffle=True, num_workers=Workers, pin_memory=True)\n",
    "# valid_data_loader = DataLoader(Image(valid_path), batch_size=BatchSize, shuffle=True, num_workers=Workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: 讀取 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Models\n",
      "Generator(\n",
      "  (Conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (Conv3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (ReLU): ReLU()\n",
      "  (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (Tanh): Tanh()\n",
      "  (RUnit1): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit2): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit3): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit4): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit5): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit6): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit7): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit8): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit9): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit10): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit11): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit12): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit13): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit14): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit15): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "  (RUnit16): Residual_Unit_GN(\n",
      "    (Conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (Conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "  (LReLU2): LeakyReLU(negative_slope=0.01)\n",
      "  (Flatten): Flatten()\n",
      "  (Dense): Linear(in_features=8192, out_features=1, bias=True)\n",
      "  (Sigmoid): Sigmoid()\n",
      "  (CUnit1): Conv_Unit_DN(\n",
      "    (Conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "    (LReLU2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (CUnit2): Conv_Unit_DN(\n",
      "    (Conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "    (LReLU2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (CUnit3): Conv_Unit_DN(\n",
      "    (Conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "    (LReLU2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (CUnit4): Conv_Unit_DN(\n",
      "    (Conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "    (LReLU2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (CUnit5): Conv_Unit_DN(\n",
      "    (Conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "    (LReLU2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (CUnit6): Conv_Unit_DN(\n",
      "    (Conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "    (LReLU2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (RUnit1): Residual_Unit_DN(\n",
      "    (Conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (BN1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (BN2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LReLU1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"[*] Loading Models\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG = Generator(NGpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = Discriminator(NGpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: 初始化參數配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initialized Parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"[*] Initialized Parameters\")\n",
    "optimizer_G = torch.optim.Adam(netG.parameters(), lr=LR, betas=(Beta1, 0.999))\n",
    "optimizer_D = torch.optim.Adam(netG.parameters(), lr=LR, betas=(Beta1, 0.999))\n",
    "MSELoss = nn.MSELoss(reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: 先稍微訓練 Generative Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Start Training Generative Network\n",
      "Epoch: 2/10 Batch: 59409/63876 G Loss: 320432.8125 Target Loss: 278304.000\r"
     ]
    }
   ],
   "source": [
    "print(\"[*] Start Training Generative Network\")\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_data_loader, 0):\n",
    "        # 初始所有變數的梯度\n",
    "        dcp, ori = data['Dcp'].to(device, dtype=torch.float, non_blocking=True), data['Ori'].to(device, dtype=torch.float, non_blocking=True)\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        gen = netG(dcp)*255\n",
    "    \n",
    "        target_loss = MSELoss(dcp, ori)\n",
    "        loss_MSE = MSELoss(gen, ori)\n",
    "        loss_MSE.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        print(\"Epoch: {}/{} Batch: {}/{} G Loss: {} Target Loss: {}\".format(epoch+1, num_epochs, i+1, len(train_data_loader), loss_MSE.item(), target_loss.item()), end=\"\\r\")\n",
    "        \n",
    "        ## Store after every epoch\n",
    "        if (loss_MSE <= 2000):\n",
    "            state = {\n",
    "                'epoch': epoch+1,\n",
    "                'netG': netG.state_dict(),\n",
    "                'netD': netD.state_dict(),\n",
    "                'optimizer_G': optimizer_G.state_dict(),\n",
    "                'optimizer_D': optimizer_D.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(state, epoch+1, i, Model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: 加入 Discriminiative Network 一起訓練， 儲存權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[*] Training Stacked Network\")\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, data in enumerate(train_data_loader, 0):\n",
    "#         # 取出影像 及 生成影像實際狀態\n",
    "#         dcp, ori = data['Dcp'].to(device, dtype=torch.float, non_blocking=True), data['Ori'].to(device, dtype=torch.float, non_blocking=True)\n",
    "        \n",
    "#         #####################\n",
    "#         ## 訓練 Descriminator\n",
    "#         ####################\n",
    "#         # 初始所有變數的梯度\n",
    "#         optimizer_D.zero_grad()\n",
    "#         gen = netG(dcp)\n",
    "        \n",
    "#         # 獲取 對抗損失\n",
    "#         loss_ADV_G, loss_ADV_D = ADVLoss(netD(ori), netD(gen.detach()))\n",
    "        \n",
    "#         # 調整 Descriminator 參數\n",
    "#         loss_ADV_D.backward(retain_graph=True)\n",
    "#         optimizer_D.step()\n",
    "        \n",
    "        \n",
    "#         #################\n",
    "#         ## 訓練 Generator\n",
    "#         ################\n",
    "#         # 初始所有變數的梯度\n",
    "#         optimizer_G.zero_grad()\n",
    "        \n",
    "#         # 獲取 Generator 的綜合損失\n",
    "#         loss_MSE = MSELoss(gen, ori)\n",
    "#         loss_Mix = loss_MSE + loss_ADV_G\n",
    "        \n",
    "#         # 調整 Generator 參數\n",
    "#         loss_Mix.backward()\n",
    "#         optimizer_G.step()\n",
    "        \n",
    "        \n",
    "#         print(\"Epoch: {}/{} Batch: {}/{} G Loss: {} D Loss: {}\".format(epoch+1, num_epochs, i+1, len(train_data_loader), loss_Mix.item(), loss_ADV_D.item()), end=\"\\r\")\n",
    "\n",
    "\n",
    "# # Step : 儲存 Final 權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.dcp_dir = os.path.join(root_dir, \"JPG\")\n",
    "        self.ori_dir = os.path.join(root_dir, \"Original\")\n",
    "        self.file_names = os.listdir(self.dcp_dir)[:1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        file_name = self.file_names[idx].split(\".\")[0]\n",
    "        img_dcp_loc = os.path.join(self.dcp_dir, \"{}.jpg\".format(file_name))\n",
    "        img_ori_loc = os.path.join(self.ori_dir, \"{}.png\".format(file_name))\n",
    "        img_dcp = cv2.imread(img_dcp_loc, 0).reshape(1,64,64)\n",
    "        img_ori = cv2.imread(img_ori_loc, 0).reshape(1,64,64)\n",
    "        sample = {'Dcp': img_dcp, 'Ori': img_ori}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_path = \"/home/mj/HardDisk/Github/Image_Compressor/Dataset/Validation_Data\"  ## 原圖影像路徑\n",
    "valid_data_loader = DataLoader(Test(valid_path), batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG = Generator(1).to(device)\n",
    "state = torch.load('/home/mj/HardDisk/Github/Image_Compressor/Model/Epoch_1_10000.pth')\n",
    "netG.load_state_dict(state['netG'])\n",
    "\n",
    "MSELoss = nn.MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for i, data in enumerate(valid_data_loader, 0):\n",
    "    dcp, ori = data['Dcp'].to(device, dtype=torch.float, non_blocking=True), data['Ori'].to(device, dtype=torch.float, non_blocking=True)\n",
    "    \n",
    "    gen = netG(dcp) * 255\n",
    "    loss_MSE = MSELoss(gen, ori)\n",
    "    print(loss_MSE)\n",
    "\n",
    "\n",
    "\n",
    "# # cv2.imshow(\"Img\", dep_img)\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# state = torch.load('/home/mj/HardDisk/Github/Image_Compressor/Model/Epoch_2.pth')\n",
    "# print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcp = cv2.imread(\"/home/mj/HardDisk/Github/Image_Compressor/Dataset/Validation_Data/Decompressed/00000001.pgm\", 0)\n",
    "# ori = cv2.imread(\"/home/mj/HardDisk/Github/Image_Compressor/Dataset/Validation_Data/Original_pgm/00000001.pgm\", 0)\n",
    "\n",
    "# print(dcp)\n",
    "# print(ori)\n",
    "\n",
    "# # cv2.imshow(\"DEP\", dcp)\n",
    "# # cv2.imshow(\"ORI\", ori)\n",
    "\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
